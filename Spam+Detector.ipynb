{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPAM Ham Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Reading the given dataset\n",
    "spam = pd.read_csv(\"SMSSpamCollection.txt\", sep = \"\\t\", names=[\"label\", \"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "print(spam.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Converting the read dataset in to a list of tuples, each tuple(row) contianing the message and it's label\n",
    "data_set = []\n",
    "for index,row in spam.iterrows():\n",
    "    data_set.append((row['message'], row['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'ham'), ('Ok lar... Joking wif u oni...', 'ham'), (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", 'spam'), ('U dun say so early hor... U c already then say...', 'ham'), (\"Nah I don't think he goes to usf, he lives around here though\", 'ham')]\n"
     ]
    }
   ],
   "source": [
    "print(data_set[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572\n"
     ]
    }
   ],
   "source": [
    "print(len(data_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## initialise the inbuilt Stemmer and the Lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(document, stem=True):\n",
    "    'changes document to lower case, removes stopwords and lemmatizes/stems the remainder of the sentence'\n",
    "    \n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "\n",
    "    if stem:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    else:\n",
    "        words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## - Performing the preprocessing steps on all messages\n",
    "messages_set = []\n",
    "for (message, label) in data_set:\n",
    "    words_filtered = [e.lower() for e in preprocess(message, stem=False).split() if len(e) >= 3]\n",
    "    messages_set.append((words_filtered, label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['jurong', 'point', 'crazy..', 'available', 'bugis', 'great', 'world', 'buffet', '...', 'cine', 'get', 'amore', 'wat', '...'], 'ham'), (['lar', '...', 'joke', 'wif', 'oni', '...'], 'ham'), (['free', 'entry', 'wkly', 'comp', 'win', 'cup', 'final', 'tkts', '21st', 'may', '2005.', 'text', '87121', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'apply', '08452810075over18'], 'spam'), (['dun', 'say', 'early', 'hor', '...', 'already', 'say', '...'], 'ham'), (['nah', \"n't\", 'think', 'usf', 'live', 'around', 'though'], 'ham')]\n"
     ]
    }
   ],
   "source": [
    "print(messages_set[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing to create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## - creating a single list of all words in the entire dataset for feature list creation\n",
    "\n",
    "def get_words_in_messages(messages):\n",
    "    all_words = []\n",
    "    for (message, label) in messages:\n",
    "      all_words.extend(message)\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## - creating a final feature list using an intuitive FreqDist, to eliminate all the duplicate words\n",
    "## Note : we can use the Frequency Distribution of the entire dataset to calculate Tf-Idf scores like we did earlier.\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "\n",
    "    #print(wordlist[:10])\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8393\n"
     ]
    }
   ],
   "source": [
    "## - creating the word features for the entire dataset\n",
    "word_features = get_word_features(get_words_in_messages(messages_set))\n",
    "print(len(word_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(word_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing to create a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## - creating slicing index at 80% threshold\n",
    "sliceIndex = int((len(messages_set)*.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## - shuffle the pack to create a random and unbiased split of the dataset\n",
    "random.shuffle(messages_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_messages, test_messages = messages_set[:sliceIndex], messages_set[sliceIndex:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_messages)\n",
    "len(test_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing to create feature maps for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## creating a LazyMap of feature presence for each of the 8K+ features with respect to each of the SMS messages\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## - creating the feature map of train and test data\n",
    "\n",
    "training_set = nltk.classify.apply_features(extract_features, train_messages)\n",
    "testing_set = nltk.classify.apply_features(extract_features, test_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(training_set[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size :  4457\n",
      "Test set size :  1115\n"
     ]
    }
   ],
   "source": [
    "print('Training set size : ', len(training_set))\n",
    "print('Test set size : ', len(testing_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Training the classifier with NaiveBayes algorithm\n",
    "spamClassifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9921471842046219\n"
     ]
    }
   ],
   "source": [
    "## - Analyzing the accuracy of the train set\n",
    "print(nltk.classify.accuracy(spamClassifier, training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979372197309417\n"
     ]
    }
   ],
   "source": [
    "## Analyzing the accuracy of the test set\n",
    "print(nltk.classify.accuracy(spamClassifier, testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification result :  spam\n"
     ]
    }
   ],
   "source": [
    "## Testing a example message with our newly trained classifier\n",
    "m = 'CONGRATULATIONS!! As a valued account holder you have been selected to receive a £900 prize reward! Valid 12 hours only.'\n",
    "print('Classification result : ', spamClassifier.classify(extract_features(m.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "         contains(award) = True             spam : ham    =    212.0 : 1.0\n",
      "         contains(nokia) = True             spam : ham    =    170.0 : 1.0\n",
      "        contains(urgent) = True             spam : ham    =    132.3 : 1.0\n",
      "       contains(service) = True             spam : ham    =    110.7 : 1.0\n",
      "          contains(code) = True             spam : ham    =    102.9 : 1.0\n",
      "         contains(video) = True             spam : ham    =     94.5 : 1.0\n",
      "           contains(txt) = True             spam : ham    =     84.5 : 1.0\n",
      "       contains(attempt) = True             spam : ham    =     77.7 : 1.0\n",
      "          contains(rate) = True             spam : ham    =     73.5 : 1.0\n",
      "      contains(delivery) = True             spam : ham    =     73.5 : 1.0\n",
      "      contains(landline) = True             spam : ham    =     66.8 : 1.0\n",
      "          contains(land) = True             spam : ham    =     65.1 : 1.0\n",
      "        contains(camera) = True             spam : ham    =     64.2 : 1.0\n",
      "           contains(100) = True             spam : ham    =     60.9 : 1.0\n",
      "     contains(statement) = True             spam : ham    =     56.7 : 1.0\n",
      "contains(congratulations) = True             spam : ham    =     56.7 : 1.0\n",
      "        contains(latest) = True             spam : ham    =     54.9 : 1.0\n",
      "          contains(draw) = True             spam : ham    =     54.9 : 1.0\n",
      "          contains(quiz) = True             spam : ham    =     52.5 : 1.0\n",
      "       contains(voucher) = True             spam : ham    =     52.5 : 1.0\n",
      "           contains(opt) = True             spam : ham    =     48.3 : 1.0\n",
      "           contains(mat) = True             spam : ham    =     48.3 : 1.0\n",
      "        contains(mobile) = True             spam : ham    =     43.1 : 1.0\n",
      "        contains(orange) = True             spam : ham    =     40.5 : 1.0\n",
      "        contains(caller) = True             spam : ham    =     39.9 : 1.0\n",
      "      contains(motorola) = True             spam : ham    =     39.9 : 1.0\n",
      "         contains(apply) = True             spam : ham    =     36.9 : 1.0\n",
      "         contains(final) = True             spam : ham    =     36.5 : 1.0\n",
      "        contains(txting) = True             spam : ham    =     35.7 : 1.0\n",
      "          contains(user) = True             spam : ham    =     35.7 : 1.0\n",
      "        contains(follow) = True             spam : ham    =     35.7 : 1.0\n",
      "       contains(auction) = True             spam : ham    =     35.7 : 1.0\n",
      "           contains(box) = True             spam : ham    =     34.3 : 1.0\n",
      "        contains(select) = True             spam : ham    =     34.3 : 1.0\n",
      "          contains(info) = True             spam : ham    =     34.0 : 1.0\n",
      "      contains(customer) = True             spam : ham    =     33.7 : 1.0\n",
      "        contains(reward) = True             spam : ham    =     31.5 : 1.0\n",
      "      contains(sunshine) = True             spam : ham    =     31.5 : 1.0\n",
      "         contains(music) = True             spam : ham    =     29.7 : 1.0\n",
      "       contains(contact) = True             spam : ham    =     29.1 : 1.0\n",
      "        contains(player) = True             spam : ham    =     29.0 : 1.0\n",
      "          contains(sony) = True             spam : ham    =     27.3 : 1.0\n",
      "         contains(offer) = True             spam : ham    =     27.3 : 1.0\n",
      "           contains(dvd) = True             spam : ham    =     27.3 : 1.0\n",
      "       contains(network) = True             spam : ham    =     26.9 : 1.0\n",
      "        contains(direct) = True             spam : ham    =     26.5 : 1.0\n",
      "         contains(pound) = True             spam : ham    =     25.9 : 1.0\n",
      "          contains(cash) = True             spam : ham    =     25.4 : 1.0\n",
      "        contains(member) = True             spam : ham    =     23.1 : 1.0\n",
      "       contains(england) = True             spam : ham    =     23.1 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Priting the most informative features in the classifier\n",
    "print(spamClassifier.show_most_informative_features(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier stored at  nb_spam_classifier.pickle\n"
     ]
    }
   ],
   "source": [
    "## storing the classifier on disk for later usage\n",
    "import pickle\n",
    "f = open('nb_spam_classifier.pickle', 'wb')\n",
    "pickle.dump(spamClassifier,f)\n",
    "print('Classifier stored at ', f.name)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
